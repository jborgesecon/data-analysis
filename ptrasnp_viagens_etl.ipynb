{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libs (navigation is a local .py file with helper functions)\n",
    "\n",
    "import navigation as nav\n",
    "import pandas as pd\n",
    "import glob\n",
    "import zipfile\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path of .zip files, add the path names to a list\n",
    "\n",
    "path = \"other_datasets\\\\ptransp_viagens\\\\*\"\n",
    "files = glob.glob(path)\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the files and add them to a pandas dataframe, then add all to df (final)\n",
    "\n",
    "for file in files:\n",
    "    with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "        datasets = zip_ref.namelist()\n",
    "        with zip_ref.open(datasets[-1]) as my_file:  # important, for we only want the last .csv withing each .zip\n",
    "            df1 = pd.read_csv(my_file, encoding='latin1', sep=';', dtype='object')\n",
    "            df = pd.concat([df, df1], ignore_index=True)\n",
    "            print(f'{file}: ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to rename the columns (use print(df.info()) to see the current col names if needed\n",
    "\n",
    "col_remap = [\n",
    "    'id_viagem',                # int\n",
    "    'num_proposta',             # int\n",
    "    'situacao',                 # obj\n",
    "    'viagem_urgente',           # bool\n",
    "    'justificativa_urgencia',   # obj\n",
    "    'cod_orgao_superior',       # int\n",
    "    'nome_orgao_superior',      # obj\n",
    "    'cod_orgao_solicitante',    # int\n",
    "    'nome_orgao_solicitante',   # obj\n",
    "    'cpf_viajante',             # obj\n",
    "    'nome_viajante',            # obj\n",
    "    'cargo',                    # obj\n",
    "    'funcao',                   # obj\n",
    "    'descricao_funcao',         # obj\n",
    "    'dt_inicio',                # datetime\n",
    "    'dt_fim',                   # datetime\n",
    "    'destinos',                 # obj\n",
    "    'motivo',                   # obj\n",
    "    'valor_diarias',            # float\n",
    "    'valor_passagens',          # float\n",
    "    'valor_devolucao',          # float\n",
    "    'valor_outros_gastos'       # float\n",
    "]\n",
    "\n",
    "df.columns = col_remap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DATA TREATMENT\n",
    "\n",
    "# normalize objects: all uppercase, no special characters\n",
    "treated_colums1 = [\n",
    "    'situacao',                 # obj\n",
    "    'justificativa_urgencia',   # obj\n",
    "    'nome_orgao_superior',      # obj\n",
    "    'nome_orgao_solicitante',   # obj\n",
    "    'cpf_viajante',             # obj\n",
    "    'nome_viajante',            # obj\n",
    "    'cargo',                    # obj\n",
    "    'funcao',                   # obj\n",
    "    'descricao_funcao',         # obj\n",
    "    'destinos',                 # obj\n",
    "    'motivo'                    # obj\n",
    "]\n",
    "\n",
    "def obj_normalizer(row):\n",
    "    if pd.isna(row):\n",
    "        return row\n",
    "    try:\n",
    "        normalized = unidecode(row).upper()\n",
    "        return normalized\n",
    "    except Exception as e:\n",
    "        print(f'Error occured: {e}')\n",
    "        return row\n",
    "\n",
    "df[treated_colums1] = df[treated_colums1].map(obj_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming from obj to datetime\n",
    "\n",
    "treated_colums2 = [\n",
    "    'dt_inicio',                # datetime\n",
    "    'dt_fim',                   # datetime\n",
    "]\n",
    "\n",
    "def date_treatment(row):\n",
    "    try:\n",
    "        return pd.to_datetime(row, dayfirst=True)\n",
    "    except ValueError as e:\n",
    "        print(f'Error on {row}: {e}')\n",
    "        return row\n",
    "df[treated_colums2] = df[treated_colums2].map(date_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming from obj to Int64\n",
    "\n",
    "treated_colums4 = [\n",
    "    'id_viagem',\n",
    "    'cod_orgao_superior',\n",
    "    'cod_orgao_solicitante'\n",
    "]\n",
    "\n",
    "for col in treated_colums4:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(-1).astype('Int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming from obj to float\n",
    "\n",
    "treated_colums3 = [\n",
    "    'valor_diarias',            # float\n",
    "    'valor_passagens',          # float\n",
    "    'valor_devolucao',          # float\n",
    "    'valor_outros_gastos'       # float\n",
    "]\n",
    "\n",
    "for col in treated_colums3:\n",
    "    df[col] = df[col].str.replace(',', '.').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming from obj to bool\n",
    "\n",
    "df['viagem_urgente'] = df['viagem_urgente'].map({'SIM': 1, 'N√ÉO': 0})\n",
    "df['viagem_urgente'] = df['viagem_urgente'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if properly treated\n",
    "\n",
    "print(df.head(5))\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_sql_in_chunks(df, table_name, conn, chunksize=100000):\n",
    "    \"\"\"\n",
    "    Save a large DataFrame to a SQL table in chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame\n",
    "        The DataFrame to be saved.\n",
    "    - table_name: str\n",
    "        The name of the table in the database.\n",
    "    - conn: SQLAlchemy or sqlite3 connection\n",
    "        The database connection.\n",
    "    - chunksize: int, optional (default=100000)\n",
    "        The number of rows per chunk to write to the database.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "    try:\n",
    "        for i in range(0, len(df), chunksize):\n",
    "            chunk = df.iloc[i:i+chunksize]\n",
    "            # Append each chunk to the table\n",
    "            chunk.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "            print(f\"Chunk {i // chunksize + 1} written to table {table_name}\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to sqlite3 database\n",
    "\n",
    "save_to_sql_in_chunks(df, 'ptransp.viagens', nav.sqlite_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amelia-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
